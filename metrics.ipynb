{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c588ce7-ed6e-417e-8978-7a3e3bc753c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_fid.fid_score import calculate_fid_given_paths\n",
    "from torch_fidelity import calculate_metrics\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "469d07ab-e206-4d5c-88b7-ad5de5f29af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['2048', 'logits_unbiased']\n",
      "Extracting features from input1\n",
      "Looking for samples non-recursivelty in \"./AC-GAN_With_Data_Enhance/gen_imgs\" with extensions png,jpg,jpeg\n",
      "Found 10000 samples\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_fidelity\\datasets.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(height, width, 3)\n",
      "Processing samples                                                                                                     \n",
      "Extracting features from input2\n",
      "Loading cached C:\\Users\\Administrator/.cache\\torch\\fidelity_cache\\cifar10-val-inception-v3-compat-features-2048.pt\n",
      "Loading cached C:\\Users\\Administrator/.cache\\torch\\fidelity_cache\\cifar10-val-inception-v3-compat-features-logits_unbiased.pt\n",
      "Inception Score: 4.676308749925367 ± 0.08646942196780155\n",
      "Loading cached C:\\Users\\Administrator/.cache\\torch\\fidelity_cache\\cifar10-val-inception-v3-compat-stat-fid-2048.pt\n",
      "Frechet Inception Distance: 76.67982087541668\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inception_score_mean': 4.676308749925367, 'inception_score_std': 0.08646942196780155, 'frechet_inception_distance': 76.67982087541668, 'kernel_inception_distance_mean': 0.06103990195195195, 'kernel_inception_distance_std': 0.0014738427729828123}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kernel Inception Distance: 0.06103990195195195 ± 0.0014738427729828123\n"
     ]
    }
   ],
   "source": [
    "gen_imgs_path = \"./AC-GAN_With_Data_Enhance/gen_imgs\"\n",
    "metrics_dict = calculate_metrics(\n",
    "    input1=gen_imgs_path, \n",
    "    input2='cifar10-val', \n",
    "    cuda=True, \n",
    "    isc=True, \n",
    "    fid=True, \n",
    "    kid=True, \n",
    "    prc=True, \n",
    "    verbose=True,\n",
    ")\n",
    "print(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79c627a-9dcf-4724-a876-394bc5d0d030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['logits_unbiased', '2048']\n",
      "Extracting features from input1\n",
      "Looking for samples non-recursivelty in \"./WGAN_With_Data_Enhance/gen_imgs\" with extensions png,jpg,jpeg\n",
      "Found 10000 samples\n",
      "Processing samples                                                                                                     \n",
      "Extracting features from input2\n",
      "Loading cached C:\\Users\\Administrator/.cache\\torch\\fidelity_cache\\cifar10-val-inception-v3-compat-features-logits_unbiased.pt\n",
      "Loading cached C:\\Users\\Administrator/.cache\\torch\\fidelity_cache\\cifar10-val-inception-v3-compat-features-2048.pt\n",
      "Inception Score: 3.450351977068096 ± 0.07281804879624017\n",
      "Loading cached C:\\Users\\Administrator/.cache\\torch\\fidelity_cache\\cifar10-val-inception-v3-compat-stat-fid-2048.pt\n",
      "Frechet Inception Distance: 104.50800524503597\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inception_score_mean': 3.450351977068096, 'inception_score_std': 0.07281804879624017, 'frechet_inception_distance': 104.50800524503597, 'kernel_inception_distance_mean': 0.08372799184434433, 'kernel_inception_distance_std': 0.002386120326794586}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kernel Inception Distance: 0.08372799184434433 ± 0.002386120326794586\n"
     ]
    }
   ],
   "source": [
    "gen_imgs_path = \"./WGAN_With_Data_Enhance/gen_imgs\"\n",
    "metrics_dict = calculate_metrics(\n",
    "    input1=gen_imgs_path, \n",
    "    input2='cifar10-val', \n",
    "    cuda=True, \n",
    "    isc=True, \n",
    "    fid=True, \n",
    "    kid=True, \n",
    "    prc=True, \n",
    "    verbose=True,\n",
    ")\n",
    "print(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39a26e34-71c4-4b56-aba5-666fa97e7119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['logits_unbiased', '2048']\n",
      "Extracting features from input1\n",
      "Looking for samples non-recursivelty in \"./Ori_GAN_With_Data_Enhance/gen_imgs\" with extensions png,jpg,jpeg\n",
      "Found 10000 samples\n",
      "Processing samples                                                                                                     \n",
      "Extracting features from input2\n",
      "Loading cached C:\\Users\\Administrator/.cache\\torch\\fidelity_cache\\cifar10-val-inception-v3-compat-features-logits_unbiased.pt\n",
      "Loading cached C:\\Users\\Administrator/.cache\\torch\\fidelity_cache\\cifar10-val-inception-v3-compat-features-2048.pt\n",
      "Inception Score: 4.335160316792999 ± 0.11022649804055462\n",
      "Loading cached C:\\Users\\Administrator/.cache\\torch\\fidelity_cache\\cifar10-val-inception-v3-compat-stat-fid-2048.pt\n",
      "Frechet Inception Distance: 79.75957089824522\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inception_score_mean': 4.335160316792999, 'inception_score_std': 0.11022649804055462, 'frechet_inception_distance': 79.75957089824522, 'kernel_inception_distance_mean': 0.06002674125875876, 'kernel_inception_distance_std': 0.0018123517182341033}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kernel Inception Distance: 0.06002674125875876 ± 0.0018123517182341033\n"
     ]
    }
   ],
   "source": [
    "gen_imgs_path = \"./Ori_GAN_With_Data_Enhance/gen_imgs\"\n",
    "metrics_dict = calculate_metrics(\n",
    "    input1=gen_imgs_path, \n",
    "    input2='cifar10-val', \n",
    "    cuda=True, \n",
    "    isc=True, \n",
    "    fid=True, \n",
    "    kid=True, \n",
    "    prc=True, \n",
    "    verbose=True,\n",
    ")\n",
    "print(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77957dc-88dc-4c5a-b34c-fba0267ecec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
